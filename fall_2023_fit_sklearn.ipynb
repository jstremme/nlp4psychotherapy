{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea00513",
   "metadata": {},
   "source": [
    "### Train Scikit-Learn Models\n",
    "\n",
    "- Joel Stremmel\n",
    "- 11-20-23\n",
    "\n",
    "##### About\n",
    "\n",
    "Train Scikit-Learn models on the formatted data using K-Fold Cross-Validation and save the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2c7a9",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd145b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nlpaug.augmenter.word import WordEmbsAug\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d89e2d",
   "metadata": {},
   "source": [
    "##### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfa5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'cohesion' # 'cohesion'\n",
    "params = {\n",
    "    \"tfidf\": {\"ngram_range\": (2, 4), \"min_df\": 5, \"max_df\": 0.9, \"sublinear_tf\": True},\n",
    "    \"models\": {\n",
    "        \"lr\": {\n",
    "            \"C\": 0.1,\n",
    "            \"seed\": 42,\n",
    "            \"max_iter\": 1000,\n",
    "            \"penalty\": \"l2\",\n",
    "            \"fit_intercept\": True,\n",
    "            \"solver\": \"saga\",\n",
    "        },\n",
    "        \"knn\": {\"n_neighbors\": 3, \"n_jobs\": 1},\n",
    "        \"gd_svm\": {\n",
    "            \"loss\": \"hinge\",\n",
    "            \"penalty\": \"l2\",\n",
    "            \"fit_intercept\": True,\n",
    "            \"max_iter\": 1000,\n",
    "            \"learning_rate\": \"optimal\",\n",
    "        },\n",
    "        \"nb\": {},\n",
    "    },\n",
    "    \"io\": {\"results_dir\": \"./results\", \"input_dir\": \"./data\"},\n",
    "    \"data\": {\n",
    "        \"sep2newlines\": True,\n",
    "        \"sep_token\": \"</s>\",\n",
    "        \"double_newlines\": \"\\n\\n\",\n",
    "    },\n",
    "    \"augmentation\": {\n",
    "        \"add_synthetic\": False,\n",
    "        \"aug_p\": 0.2,\n",
    "        \"glove_file\": \"data/glove.6B.50d.txt\",\n",
    "        \"glove_zip\": \"data/glove.6B.zip\",\n",
    "        \"glove_url\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    },\n",
    "    \"random\": {\"seed\": 42},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78b64c",
   "metadata": {},
   "source": [
    "##### Load Formatted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710a85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(params[\"io\"][\"input_dir\"], f\"{outcome}_X_folds.pkl\"), \"rb\") as f:\n",
    "    X_folds = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(params[\"io\"][\"input_dir\"], f\"{outcome}_y_folds.pkl\"), \"rb\") as f:\n",
    "    y_folds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7454ffd",
   "metadata": {},
   "source": [
    "##### Define a Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2eb1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    with open(filename, \"wb\") as file:\n",
    "        response = requests.get(url)\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77868f",
   "metadata": {},
   "source": [
    "##### Download GloVe Embeddings if Non-Existent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40151493",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(params[\"augmentation\"][\"glove_file\"]):\n",
    "    print(\"Downloading glove embeddings file...\")\n",
    "    os.makedirs(params[\"io\"][\"input_dir\"], exist_ok=True)\n",
    "\n",
    "    download_file(url, params[\"augmentation\"][\"glove_zip\"])\n",
    "\n",
    "    with zipfile.ZipFile(params[\"augmentation\"][\"glove_zip\"], \"r\") as zip_ref:\n",
    "        zip_ref.extractall(params[\"io\"][\"input_dir\"])\n",
    "\n",
    "    os.remove(params[\"augmentation\"][\"glove_zip\"])\n",
    "    print(\"Finished downloading glove embeddings file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b559a",
   "metadata": {},
   "source": [
    "##### Check Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7c83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_folds) == len(y_folds), \"Expected the same number of folds in X and y.\"\n",
    "X = list(X_folds.values())\n",
    "y = list(y_folds.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb255f",
   "metadata": {},
   "source": [
    "##### Check Number of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51ef687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 117.\n"
     ]
    }
   ],
   "source": [
    "num_samples = len([x for xx in y for x in xx])\n",
    "print(f\"Total number of samples: {num_samples}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7cd9d",
   "metadata": {},
   "source": [
    "##### Check Target Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2164340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prevalance: 0.607.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target prevalance: {round(np.mean(np.concatenate(y)), 3)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2951e",
   "metadata": {},
   "source": [
    "##### Vectorize Text and Fit Model to Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cc6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model: lr using fold 0 as out of fold data.\n",
      "Fitting model: lr using fold 1 as out of fold data.\n",
      "Fitting model: lr using fold 2 as out of fold data.\n",
      "Fitting model: lr using fold 3 as out of fold data.\n",
      "Fitting model: lr using fold 4 as out of fold data.\n",
      "Fitting model: lr using fold 5 as out of fold data.\n",
      "Fitting model: lr using fold 6 as out of fold data.\n",
      "Fitting model: lr using fold 7 as out of fold data.\n",
      "Fitting model: lr using fold 8 as out of fold data.\n",
      "Fitting model: lr using fold 9 as out of fold data.\n",
      "Fitting model: lr using fold 10 as out of fold data.\n",
      "Fitting model: knn using fold 0 as out of fold data.\n",
      "Fitting model: knn using fold 1 as out of fold data.\n",
      "Fitting model: knn using fold 2 as out of fold data.\n",
      "Fitting model: knn using fold 3 as out of fold data.\n",
      "Fitting model: knn using fold 4 as out of fold data.\n",
      "Fitting model: knn using fold 5 as out of fold data.\n",
      "Fitting model: knn using fold 6 as out of fold data.\n",
      "Fitting model: knn using fold 7 as out of fold data.\n",
      "Fitting model: knn using fold 8 as out of fold data.\n",
      "Fitting model: knn using fold 9 as out of fold data.\n",
      "Fitting model: knn using fold 10 as out of fold data.\n",
      "Fitting model: gd_svm using fold 0 as out of fold data.\n",
      "Fitting model: gd_svm using fold 1 as out of fold data.\n",
      "Fitting model: gd_svm using fold 2 as out of fold data.\n",
      "Fitting model: gd_svm using fold 3 as out of fold data.\n",
      "Fitting model: gd_svm using fold 4 as out of fold data.\n",
      "Fitting model: gd_svm using fold 5 as out of fold data.\n",
      "Fitting model: gd_svm using fold 6 as out of fold data.\n",
      "Fitting model: gd_svm using fold 7 as out of fold data.\n",
      "Fitting model: gd_svm using fold 8 as out of fold data.\n",
      "Fitting model: gd_svm using fold 9 as out of fold data.\n",
      "Fitting model: gd_svm using fold 10 as out of fold data.\n",
      "Fitting model: nb using fold 0 as out of fold data.\n",
      "Fitting model: nb using fold 1 as out of fold data.\n",
      "Fitting model: nb using fold 2 as out of fold data.\n",
      "Fitting model: nb using fold 3 as out of fold data.\n",
      "Fitting model: nb using fold 4 as out of fold data.\n",
      "Fitting model: nb using fold 5 as out of fold data.\n",
      "Fitting model: nb using fold 6 as out of fold data.\n",
      "Fitting model: nb using fold 7 as out of fold data.\n",
      "Fitting model: nb using fold 8 as out of fold data.\n",
      "Fitting model: nb using fold 9 as out of fold data.\n",
      "Fitting model: nb using fold 10 as out of fold data.\n"
     ]
    }
   ],
   "source": [
    "y_probs, y_trues = {}, {}\n",
    "for model in params[\"models\"].keys():\n",
    "    y_probs[model], y_trues[model] = [], []\n",
    "    for i in range(len(X)):\n",
    "        print(f\"Fitting model: {model} using fold {i} as out of fold data.\")\n",
    "\n",
    "        # Define TFIDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=params[\"tfidf\"][\"sublinear_tf\"],\n",
    "            strip_accents=\"unicode\",\n",
    "            analyzer=\"word\",\n",
    "            token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=params[\"tfidf\"][\"ngram_range\"],\n",
    "            norm=\"l2\",\n",
    "            min_df=params[\"tfidf\"][\"min_df\"],\n",
    "            max_df=params[\"tfidf\"][\"max_df\"],\n",
    "            smooth_idf=False,\n",
    "            lowercase=True,\n",
    "        )\n",
    "\n",
    "        # Identify train folds and shuffle samples\n",
    "        X_train, y_train = np.concatenate(X[0:i] + X[i + 1 :], axis=0), np.concatenate(\n",
    "            y[0:i] + y[i + 1 :], axis=0\n",
    "        )\n",
    "        indices = np.arange(len(y_train))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        # Identify test folds\n",
    "        X_test, y_test = X[i], y[i]\n",
    "\n",
    "        # Optionally replace sep token with double newlines\n",
    "        if params[\"data\"][\"sep2newlines\"]:\n",
    "            X_train = np.array(\n",
    "                [\n",
    "                    sample.replace(\n",
    "                        params[\"data\"][\"sep_token\"], params[\"data\"][\"double_newlines\"]\n",
    "                    )\n",
    "                    for sample in X_train\n",
    "                ]\n",
    "            )\n",
    "            X_test = np.array(\n",
    "                [\n",
    "                    sample.replace(\n",
    "                        params[\"data\"][\"sep_token\"], params[\"data\"][\"double_newlines\"]\n",
    "                    )\n",
    "                    for sample in X_test\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Vectorize text\n",
    "        train_features = tfidf_vectorizer.fit_transform(X_train)\n",
    "        test_features = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "        # Optionally add synthetic samples\n",
    "        if params[\"augmentation\"][\"add_synthetic\"]:\n",
    "            # Augment train data with synthetic text using WordEmbsAug\n",
    "            # .ContextualWordEmbsAug(model_path=\"roberta-large\", action=\"insert\")\n",
    "            aug = WordEmbsAug(\n",
    "                model_type=\"glove\",\n",
    "                model_path=params[\"augmenation\"][\"glove_file\"],\n",
    "                action=\"insert\",\n",
    "                aug_p=params[\"augmenation\"][\"aug_p\"],\n",
    "            )\n",
    "\n",
    "            # Only augment non-empty samples\n",
    "            # In general, we should check for empty samples and possibly remove them\n",
    "            X_train_aug = []\n",
    "            y_train_aug = []\n",
    "            for sample, target in zip(X_train, y_train):\n",
    "                if sample != \"\":\n",
    "                    aug_sample = aug.augment(sample)[0]\n",
    "                    X_train_aug.append(aug_sample)\n",
    "                    y_train_aug.append(target)\n",
    "\n",
    "            # Vectorize synthetic text\n",
    "            train_features_aug = tfidf_vectorizer.transform(np.array(X_train_aug))\n",
    "\n",
    "            # Combine original and synthetic text\n",
    "            train_features = sp.sparse.vstack([train_features, train_features_aug])\n",
    "            y_train = np.concatenate((y_train, y_train_aug), axis=0)\n",
    "\n",
    "            # Shuffle combined training data\n",
    "            new_indices = np.arange(len(y_train))\n",
    "            np.random.shuffle(new_indices)\n",
    "            train_features, y_train = train_features[new_indices], y_train[new_indices]\n",
    "\n",
    "        # Define model to fit\n",
    "        if model == \"lr\":\n",
    "            clf = LogisticRegression(\n",
    "                solver=params[\"models\"][model][\"solver\"],\n",
    "                fit_intercept=params[\"models\"][model][\"fit_intercept\"],\n",
    "                max_iter=params[\"models\"][model][\"max_iter\"],\n",
    "                penalty=params[\"models\"][model][\"penalty\"],\n",
    "                C=params[\"models\"][model][\"C\"],\n",
    "                class_weight=None,\n",
    "                random_state=params[\"random\"][\"seed\"],\n",
    "            )\n",
    "        elif model == \"knn\":\n",
    "            clf = KNeighborsClassifier(\n",
    "                n_neighbors=params[\"models\"][model][\"n_neighbors\"],\n",
    "                n_jobs=params[\"models\"][model][\"n_jobs\"],\n",
    "            )\n",
    "            train_features = train_features.toarray()\n",
    "            test_features = test_features.toarray()\n",
    "        elif model == \"nb\":\n",
    "            clf = GaussianNB()\n",
    "            train_features = train_features.toarray()\n",
    "            test_features = test_features.toarray()\n",
    "        elif model == \"gd_svm\":\n",
    "            clf = SGDClassifier(\n",
    "                loss=params[\"models\"][model][\"loss\"],\n",
    "                penalty=params[\"models\"][model][\"penalty\"],\n",
    "                fit_intercept=params[\"models\"][model][\"fit_intercept\"],\n",
    "                max_iter=params[\"models\"][model][\"max_iter\"],\n",
    "                learning_rate=params[\"models\"][model][\"learning_rate\"],\n",
    "                random_state=params[\"random\"][\"seed\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Got unexpected model key: {model}.\")\n",
    "\n",
    "        # Fit model\n",
    "        clf.fit(train_features, y_train)\n",
    "\n",
    "        # Predict on test folds\n",
    "        if model == \"gd_svm\":\n",
    "            y_prob = clf.predict(test_features)\n",
    "        else:\n",
    "            y_prob = clf.predict_proba(test_features)[:, 1]\n",
    "\n",
    "        # Save scores and labels\n",
    "        y_probs[model].append(y_prob)\n",
    "        y_trues[model].append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da93cef",
   "metadata": {},
   "source": [
    "##### Save Model Scores on Test Folds and True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(params[\"io\"][\"results_dir\"], f\"{outcome}_sklearn_y_trues.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_trues, f)\n",
    "\n",
    "with open(os.path.join(params[\"io\"][\"results_dir\"], f\"{outcome}_sklearn_y_probs.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81733659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts",
   "language": "python",
   "name": "transcripts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
