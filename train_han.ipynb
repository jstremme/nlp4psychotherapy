{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea00513",
   "metadata": {},
   "source": [
    "### Train Hierarchical Attention Network\n",
    "\n",
    "- Joel Stremmel\n",
    "- 04-19-23\n",
    "\n",
    "##### About\n",
    "\n",
    "Train a Hierarchical Attention Network (HAN) on the formatted data using K-Fold Cross-Validation and save the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2c7a9",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd145b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling1D, Dropout, LSTM, Bidirectional, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d89e2d",
   "metadata": {},
   "source": [
    "##### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfa5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "max_num_utterances = 32\n",
    "batch_size = 32\n",
    "accumulation_steps = 1\n",
    "lr = 0.00002\n",
    "weight_decay = 0.01\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_epsilon = 0.00000001\n",
    "warmup_steps = 2\n",
    "logging_steps = 1\n",
    "num_workers = 8\n",
    "seed = 44\n",
    "epochs = 5\n",
    "fp16 = True\n",
    "output_dir = \"lf_output\"\n",
    "lm_path = \"kiddothe2b/longformer-mini-1024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb2634",
   "metadata": {},
   "source": [
    "##### Disable Tokenizer Parallelism\n",
    "This is mostly to avoid warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13a3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78b64c",
   "metadata": {},
   "source": [
    "##### Load Formatted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710a85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X_folds.pkl', 'rb') as f:\n",
    "    X_folds = pickle.load(f)\n",
    "\n",
    "with open('data/y_folds.pkl', 'rb') as f:\n",
    "    y_folds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b559a",
   "metadata": {},
   "source": [
    "##### Check Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7c83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_folds) == len(y_folds), \"Expected the same number of folds in X and y.\"\n",
    "X = list(X_folds.values())\n",
    "y = list(y_folds.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7cd9d",
   "metadata": {},
   "source": [
    "##### Check Target Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2164340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prevalance: 0.5277777777777778.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target prevalance: {np.mean(np.concatenate(y))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff6576",
   "metadata": {},
   "source": [
    "##### Check that GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617bd083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"Run this script on a GPU.\"\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2951e",
   "metadata": {},
   "source": [
    "##### Tokenize Text and Fit Model to Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cc6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model using fold 0 as out of fold data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/longformer-mini-1024 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'longformer.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/longformer-mini-1024 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3bb758c5d04c5789f644e33b5e7ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968e7e80364040cd9275b32ea0b577fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6833, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6954, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6782, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.5}\n",
      "{'loss': 0.702, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.681, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6998, 'learning_rate': 1e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6804, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.6678, 'learning_rate': 5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6787, 'learning_rate': 2.5e-06, 'epoch': 4.5}\n",
      "{'loss': 0.6697, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 45.8534, 'train_samples_per_second': 5.997, 'train_steps_per_second': 0.218, 'train_loss': 0.6836280763149262, 'epoch': 5.0}\n",
      "Fitting model using fold 1 as out of fold data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/longformer-mini-1024 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'longformer.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/longformer-mini-1024 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f350c66a2ea04fea8fd6765f601a13a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510db7bd5f7c4cfd8ef7f27fa89f4fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.688, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.697, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6941, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6761, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6871, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6819, 'learning_rate': 1e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6816, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.6787, 'learning_rate': 5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6838, 'learning_rate': 2.5e-06, 'epoch': 4.5}\n",
      "{'loss': 0.6652, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 46.8505, 'train_samples_per_second': 6.083, 'train_steps_per_second': 0.213, 'train_loss': 0.6833590388298034, 'epoch': 5.0}\n",
      "Fitting model using fold 2 as out of fold data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/longformer-mini-1024 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'longformer.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/longformer-mini-1024 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c12be96046f45d2889023882e8e511f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53b64db45e946d987141034c3934c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6886, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6991, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
      "{'loss': 0.7014, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6754, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6921, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6911, 'learning_rate': 1e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6815, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.6825, 'learning_rate': 5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6862, 'learning_rate': 2.5e-06, 'epoch': 4.5}\n",
      "{'loss': 0.6774, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 46.9541, 'train_samples_per_second': 6.07, 'train_steps_per_second': 0.213, 'train_loss': 0.6875467717647552, 'epoch': 5.0}\n",
      "Fitting model using fold 3 as out of fold data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/longformer-mini-1024 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'longformer.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/longformer-mini-1024 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49908b3939614536b857e0a3cfe36fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5965541302450a887315aa472e2730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.698, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6824, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6982, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6812, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6981, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6845, 'learning_rate': 1e-05, 'epoch': 3.0}\n",
      "{'loss': 0.686, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.6888, 'learning_rate': 5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6715, 'learning_rate': 2.5e-06, 'epoch': 4.5}\n",
      "{'loss': 0.6844, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 49.5067, 'train_samples_per_second': 6.06, 'train_steps_per_second': 0.202, 'train_loss': 0.6873256683349609, 'epoch': 5.0}\n",
      "Fitting model using fold 4 as out of fold data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/longformer-mini-1024 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'longformer.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/longformer-mini-1024 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a92ca1b2644f25aa9c1011fb38f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a021c39b2a947749868888584281034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6848, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6852, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6851, 'learning_rate': 1.7500000000000002e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6873, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6586, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6888, 'learning_rate': 1e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6771, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.6671, 'learning_rate': 5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6821, 'learning_rate': 2.5e-06, 'epoch': 4.5}\n",
      "{'loss': 0.679, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 48.7569, 'train_samples_per_second': 6.05, 'train_steps_per_second': 0.205, 'train_loss': 0.6795224547386169, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "y_probs, y_trues = [], []\n",
    "for i in range(len(X)):\n",
    "    \n",
    "    print(f\"Fitting model using fold {i} as out of fold data.\")\n",
    "    \n",
    "    # Identify train folds and shuffle samples\n",
    "    X_train, y_train = np.concatenate(X[0:i] + X[i+1:], axis=0), np.concatenate(y[0:i] + y[i+1:], axis=0)\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train = X_train[indices], y_train[indices]\n",
    "    \n",
    "    # Identify test folds\n",
    "    X_test, y_test = X[i], y[i]\n",
    "    \n",
    "    # Format text and label data as HuggingFace dataset\n",
    "    train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "    test_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "    # Define the input layers\n",
    "    full_text_input = Input(shape=(max_seq_len,), name='full_text_input')\n",
    "    utterance_input = Input(shape=(max_num_utterances, max_seq_len), name='utterance_input')\n",
    "\n",
    "    # Define the pre-trained transformer model (e.g., BERT, RoBERTa)\n",
    "    pretrained_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define the hierarchical attention network architecture\n",
    "    full_text_encoding = pretrained_model(full_text_input)[0]\n",
    "    full_text_encoding = GlobalAveragePooling1D()(full_text_encoding)\n",
    "    full_text_encoding = Dropout(0.2)(full_text_encoding)\n",
    "\n",
    "    utterance_encodings = []\n",
    "    for i in range(max_num_utterances):\n",
    "        utterance_encoding = pretrained_model(utterance_input[:, i, :])[0]\n",
    "        utterance_encoding = Bidirectional(LSTM(64, return_sequences=True))(utterance_encoding)\n",
    "        attention_weights = Dense(1, activation='tanh')(utterance_encoding)\n",
    "        attention_weights = tf.squeeze(attention_weights, axis=-1)\n",
    "        attention_weights = tf.nn.softmax(attention_weights, axis=-1)\n",
    "        utterance_encoding = tf.matmul(tf.transpose(attention_weights, [0, 2, 1]), utterance_encoding)\n",
    "        utterance_encoding = LayerNormalization()(utterance_encoding)\n",
    "        utterance_encoding = GlobalAveragePooling1D()(utterance_encoding)\n",
    "        utterance_encoding = Dropout(0.2)(utterance_encoding)\n",
    "        utterance_encodings.append(utterance_encoding)\n",
    "\n",
    "    participant_output = Concatenate()(utterance_encodings)\n",
    "    participant_output = Dense(128, activation='relu')(participant_output)\n",
    "    participant_output = Dropout(0.2)(participant_output)\n",
    "    participant_output = Dense(1, activation='sigmoid', name='participant_output')(participant_output)\n",
    "\n",
    "    model = Model(inputs=[full_text_input, utterance_input], outputs=[participant_output])\n",
    "\n",
    "    # Compile the model with a categorical cross-entropy loss function\n",
    "    optimizer = Adam(lr=2e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        {'full_text_input': X_train_full, 'utterance_input': X_train_utterances},\n",
    "        {'participant_output': y_train},\n",
    "        validation_data=None,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        callbacks=[]\n",
    "    )\n",
    "\n",
    "    # Predict on test dataset\n",
    "    output = trainer.predict(test_dataset)\n",
    "    labels = output.label_ids\n",
    "    y_prob = model.predict({'full_text_input': X_test_full, 'utterance_input': X_test_utterances})\n",
    "\n",
    "    # Save scores and labels\n",
    "    y_probs.append(y_prob)\n",
    "    y_trues.append(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da93cef",
   "metadata": {},
   "source": [
    "##### Save Model Probabilities on Test Folds and True Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bbff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/lfm_y_trues.pkl', 'wb') as f:\n",
    "    pickle.dump(y_trues, f)\n",
    "\n",
    "with open('results/lfm_y_probs.pkl', 'wb') as f:\n",
    "    pickle.dump(y_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d8469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts",
   "language": "python",
   "name": "transcripts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
